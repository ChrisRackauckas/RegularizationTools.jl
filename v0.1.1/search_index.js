var documenterSearchIndex = {"docs":
[{"location":"references/#Referenes","page":"References","title":"Referenes","text":"","category":"section"},{"location":"references/#Credits","page":"References","title":"Credits","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"This package and the manual were strongly influenced by Hansen (2000). The image for the logo is a picture of Andrey Nikolayevich Tikhonov taken from Wikipedia. The credit for introducing the regularized solution is shared with Twomey (1963).","category":"page"},{"location":"references/#Bibliography","page":"References","title":"Bibliography","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Eldén, L. (1977) Algorithms for the regularization of ill-conditioned least squares problems, BIT 17, 134–145, DOI:10.1007/BF01932285.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Golub, G. H., Heath, M. and Wahba, G. (1979) Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter,  Technometrics 21(2), 215–223, DOI:10.1080/00401706.1979.10489751.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Hansen, P. C. (1998) 2. Decompositions and Other Tools, in Rank-Deficient and Discrete Ill-Posed Problems, 19-44, DOI:10.1137/1.9780898719697.ch2.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Hansen, P. C. (2000) The L-Curve and its Use in the Numerical Treatment of Inverse Problems, in Computational Inverse Problems in Electrocardiology, ed. P. Johnston, Advances in Computational Bioengineering, 119–142, WIT Press.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Huckle, T. and Sedlacek, M. (2012) Data Based Regularization Matrices for the Tikhonov-PhillipsRegularization, Proc. Appl. Math. Mech., 12, 643 – 644,  DOI:10.1002/pamm.201210310.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Lira, M, Iyer, R., Trinidade, A., Howle, V. (2016) QR versus Cholesky: A probabilistic analysis, International Journal of Numerical Analysis and Modeling, 13(1), 114-121.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Twomey, S. (1963) On the numerical solution of Fredholm integral equations of the first kind by inversion of the linear system produced by quadrature, Journal of the ACM, 19(1963), 97–101, DOI:10.1145/321150.321157.","category":"page"},{"location":"library/#Data-Types","page":"Library","title":"Data Types","text":"","category":"section"},{"location":"library/#RegularizationProblem","page":"Library","title":"RegularizationProblem","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"RegularizationProblem","category":"page"},{"location":"library/#RegularizationTools.RegularizationProblem","page":"Library","title":"RegularizationTools.RegularizationProblem","text":"RegularizationProblem\n\nThis data type contains the cached matrices used in the inversion. The problem is  initialized using the constructor setupRegularizationProblem with the design matrix  A and the the Tikhonv matrix L as inputs. The hat quantities, e.g. Ā, is the calculated design matrix in standard form. ĀĀ, Āᵀ, F̄ are precomputed to speed up repeating inversions with different data. L⁺ₐ is cached to speed up the repeated conversion of  data to_standard_form and to_general_form\n\nĀ::Matrix{Float64}     # Standard form of design matrix\nA::Matrix{Float64}     # General form of the design matrix (n×p)\nL::Matrix{Float64}     # Smoothing matrix (n×p)\nĀĀ::Matrix{Float64}    # Cached value of Ā'Ā for performance\nĀᵀ::Matrix{Float64}    # Cached value of Ā' for performance\nF̄::SVD                 # Cached SVD decomposition of Ā \nIₙ::Matrix{Float64}    # Cached identity matrix n×n\nIₚ::Matrix{Float64}    # Cached identity matrix p×p\nL⁺ₐ::Matrix{Float64}   # Cached A-weighted generalized inverse of L(standard-form conversion)\n\n\n\n\n\n","category":"type"},{"location":"library/#RegularizatedSolution","page":"Library","title":"RegularizatedSolution","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"RegularizedSolution","category":"page"},{"location":"library/#RegularizationTools.RegularizedSolution","page":"Library","title":"RegularizationTools.RegularizedSolution","text":"RegularizatedSolution\n\nData tpye to store the optimal solution x of the inversion. λ is the optimal λ used  solution is the raw output from the Optim search.\n\nx::AbstractVector\nλ::AbstractFloat\nsolution::Optim.UnivariateOptimizationResults\n\n\n\n\n\n","category":"type"},{"location":"library/#Constructor-Functions","page":"Library","title":"Constructor Functions","text":"","category":"section"},{"location":"library/#Tikhonov-Matrix","page":"Library","title":"Tikhonov Matrix","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"Γ","category":"page"},{"location":"library/#RegularizationTools.Γ","page":"Library","title":"RegularizationTools.Γ","text":"Γ(A::AbstractMatrix, order::Int)\n\nReturn the smoothing matrix L for zero, first and second order Tikhonov regularization  based on the size of design matrix A. Order can be 0, 1 or 2.\n\nL = Γ(A, 1)\n\n\n\n\n\n","category":"function"},{"location":"library/#setupRegularizationProblem","page":"Library","title":"setupRegularizationProblem","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"setupRegularizationProblem","category":"page"},{"location":"library/#RegularizationTools.setupRegularizationProblem","page":"Library","title":"RegularizationTools.setupRegularizationProblem","text":"setupRegularizationProblem(A::AbstractMatrix, order::Int)\n\nPrecompute matrices to initialize Reguluarization Problem based on design matrix A and  zeroth, first, or second order difference operator. See Hanson (1998) and source code for details.\n\nExample Usage\n\nΨ = setupRegularizationProblem(A, 0) # zeroth order problem\nΨ = setupRegularizationProblem(A, 2) # second order problem\n\n\n\n\n\nsetupRegularizationProblem(A::AbstractMatrix, L::AbstractMatrix)\n\nPrecompute matrices to initialize Reguluarization Problem based on design matrix and  Tikhonov smoothing matrix. See Hansen (1998, Eq. 2.35)\n\nExample Usage\n\nΨ = setupRegularizationProblem(A, L) \n\n\n\n\n\n","category":"function"},{"location":"library/#to_standard_form","page":"Library","title":"to_standard_form","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"to_standard_form","category":"page"},{"location":"library/#RegularizationTools.to_standard_form","page":"Library","title":"RegularizationTools.to_standard_form","text":"to_standard_form(Ψ::RegularizationProblem, b::AbstractVector)\n\nConverts vector b to standard form using (Hansen, 1998)\n\nExample Usage (Regular Syntax)\n\nb̄ = to_standard_form(Ψ, b)\n\nExample Usage (Lazy Syntax)\n\nb̄ = @>> b to_standard_form(Ψ)\n\n\n\n\n\nto_standard_form(Ψ::RegularizationProblem, b::AbstractVector, x₀::AbstractVector)\n\nConverts vector b and x₀ to standard form using (Hansen, 1998)\n\nExample Usage (Regular Syntax)\n\nb̄ = to_standard_form(Ψ, b, x₀)\n\n\n\n\n\n","category":"function"},{"location":"library/#to_general_form","page":"Library","title":"to_general_form","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"to_general_form","category":"page"},{"location":"library/#RegularizationTools.to_general_form","page":"Library","title":"RegularizationTools.to_general_form","text":"to_general_form(Ψ::RegularizationProblem, b::AbstractVector, x̄::AbstractVector)\n\nConverts solution bar rm x computed in standard form back to general form  rm x using (Hansen, 1998). Solution is truncated to regularized space, given by the matrix L. If L is p × n and p < n, then only the solution 1:p is valid. The remaining  parameters can be estiamted from the least-squares solution if needed.\n\nrm x=rm bf L^+_Abarx\n\nwhere the matrices and vectors are defined in RegularizationProblem\n\nExample Usage (Regular Syntax)\n\nx = to_general_form(Ψ, b, x̄) \n\nExample Usage (Lazy Syntax)\n\nx = @>> x̄ to_general_form(Ψ, b) \n\n\n\n\n\n","category":"function"},{"location":"library/#Solvers","page":"Library","title":"Solvers","text":"","category":"section"},{"location":"library/#Solve","page":"Library","title":"Solve","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"solve","category":"page"},{"location":"library/#RegularizationTools.solve","page":"Library","title":"RegularizationTools.solve","text":"solve(Ψ::RegularizationProblem, b̄::AbstractVector, λ::AbstractFloat)\n\nCompute the Tikhonov solution for problem Ψ in standard form for regularization parameter λ and using zero as initial guess. Returns a vector rm bar x_lambda. \n\nrm x_lambda=left(rm bf bar A^Trm bf bar A+lambda^2rm bf Iright)^-1 \nrm bf bar A^Trm bar b \n\nExample Usage (Standard Syntax)\n\n# A is a Matrix and b is a response vector. \nΨ = setupRegularizationProblem(A, 1)     # Setup problem\nb̄ = to_standard_form(Ψ, b)               # Convert to standard form\nx̄ = solve(A, b̄, 0.5)                     # Solve the equation\nx = to_general_form(Ψ, b, x̄)             # Convert back to general form\n\nExample Usage (Lazy Syntax)\n\n# A is a Matrix and b is a response vector. \nΨ = setupRegularizationProblem(A, 1)     # Setup problem\nb̄ = @>> b to_standard_form(Ψ)            # Convert to standard form\nx̄ = solve(A, b̄, 0.5)                     # Solve the equation\nx = @>> x̄ to_general_form(Ψ, b)          # Convert back to general form\n\n\n\n\n\nsolve(Ψ::RegularizationProblem, b̄::AbstractVector, x̄₀::AbstractVector, λ::AbstractFloat)\n\nCompute the Tikhonov solution for problem Ψ in standard form for regularization parameter λ and using x̄₀ as initial guess. \n\nrm x_lambda=left(rm bf bar A^Trm bf bar A+lambda^2rm bf Iright)^-1 \nleft(rm bf bar A^Trm bar b + lambda^2 rm bar x_0 right)\n\nExample Usage (Standard Syntax)\n\n# A is a Matrix and b is a response vector. \nΨ = setupRegularizationProblem(A, 2)     # Setup problem\nb̄, x̄₀ = to_standard_form(Ψ, b, x₀)       # Convert to standard form\nx̄ = solve(A, b̄, x̄₀, 0.5)                 # Solve the equation\nx = to_general_form(Ψ, b, x̄)             # Convert back to general form\n\n\n\n\n\nfunction solve(\n    Ψ::RegularizationProblem,\n    b::AbstractVector;\n    alg = :gcv_svd,\n    λ₁ = 0.0001,\n    λ₂ = 1000.0,\n)\n\nFind the optimum regularization parameter λ between [λ₁, λ₂] using the algorithm alg. Choices for algorithms are\n\n    :gcv_tr - generalized cross validation using the trace formulation (slow)\n    :gcv_svd - generalized cross validation using the SVD decomposition (fast)\n    :L_curve - L-curve algorithm \n\ntip: Tip\nThe gcv_svd algorithm is fastest and most stable. The L_curve algorithn is sensitive to the upper  and lower bound. Specify narrow upper and lower bounds to obtain a good solution.\n\nThe solve function takes the original data, converts it to standard form, performs the search within the specified bounds and returns a RegularizatedSolution\n\nExample Usage (Standard Syntax)\n\n# A is a Matrix and b is a response vector. \nΨ = setupRegularizationProblem(A, 2)     # Setup problem\nsol = solve(Ψ, b)                        # Solve it\n\nExample Usage (Lazy Syntax)\n\n# A is a Matrix and b is a response vector. \nsol = @> setupRegularizationProblem(A, 1) solve(b)\n\n\n\n\n\nfunction solve(\n    Ψ::RegularizationProblem,\n    b::AbstractVector,\n    x₀::AbstractVector;\n    alg = :gcv_svd,\n    λ₁ = 0.0001,\n    λ₂ = 1000.0,\n)\n\nSame as above, but includes an initial guess x₀. Example Usage (Lazy Syntax)\n\n# A is a Matrix and b is a response vector. \nsol = @> setupRegularizationProblem(A, 1) solve(b, x₀, alg = :L_curve, λ₂ = 10.0)\n\n\n\n\n\n","category":"function"},{"location":"library/#Validators","page":"Library","title":"Validators","text":"","category":"section"},{"location":"library/#GCV","page":"Library","title":"GCV","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"gcv_tr","category":"page"},{"location":"library/#RegularizationTools.gcv_tr","page":"Library","title":"RegularizationTools.gcv_tr","text":"gcv_tr(Ψ::RegularizationProblem, b̄::AbstractVector, λ::AbstractFloat)\n\nCompute the Generalized Cross Validation using the trace term. Requires that the  vector b̄ is in standard form.\n\nV(lambda)=fracnleftlVert (bf rm bf bf I-bf bar A_lambda)\nbarrm brightrVert _2^2tr(rm bf I-rm bar bf A_lambda)^2\n\nExample Usage\n\nusing Underscores\n\nΨ = setupRegularizationProblem(A, 1)           # Setup problem\nb̄ = to_standard_form(Ψ, b)                     # Convert to standard form\nVλ = gcv_tr(Ψ, b̄, 0.1)                         # V(λ) single λ value\nVλ = @_ map(gcv_tr(Ψ, b̄, _), [0.1, 1.0, 10.0]) # V(λ) for array of λ\n\n\n\n\n\ngcv_tr(\n    Ψ::RegularizationProblem,\n    b̄::AbstractVector,\n    x̄₀::AbstractVector,\n    λ::AbstractFloat,\n)\n\nCompute the Generalized Cross Validation using the trace term and intial guess.  Requires that the vectors b̄ and x̄₀ are in standard form.\n\nV(lambda)=fracnleftlVert bf rm bf barArm barx_lambda-\nrm barbrightrVert _2^2tr(rm bf I-rm bar bf A_lambda)^2\n\nExample Usage\n\nusing Underscores\n\nΨ = setupRegularizationProblem(A, 1)               # Setup problem\nb̄, x̄₀ = to_standard_form(Ψ, b, x₀)                 # Convert to standard form\nVλ = gcv_tr(Ψ, b̄, x̄₀, 0.1)                         # V(λ) single λ value\nVλ = @_ map(gcv_tr(Ψ, b̄, x̄₀, _), [0.1, 1.0, 10.0]) # V(λ) for array of λ\n\n\n\n\n\n","category":"function"},{"location":"library/","page":"Library","title":"Library","text":"gcv_svd","category":"page"},{"location":"library/#RegularizationTools.gcv_svd","page":"Library","title":"RegularizationTools.gcv_svd","text":"gcv_svd(Ψ::RegularizationProblem, b̄::AbstractVector, λ::AbstractFloat)\n\nCompute the Generalized Cross Validation using the trace term using the SVD  algorithm. Requires that the vector b̄ is in standard form.\n\nExample Usage\n\nusing Underscores\n\nΨ = setupRegularizationProblem(A, 1)            # Setup problem\nb̄, x̄₀ = to_standard_form(Ψ, b, x₀)              # Convert to standard form\nVλ = gcv_svd(Ψ, b̄, x̄₀, 0.1)                     # V(λ) single λ value\nVλ = @_ map(gcv_svd(Ψ, b̄, _), [0.1, 1.0, 10.0]) # V(λ) for array of λ\n\n\n\n\n\ngcv_svd(\n    Ψ::RegularizationProblem,\n    b̄::AbstractVector,\n    x̄₀::AbstractVector,\n    λ::AbstractFloat,\n)\n\nCompute the Generalized Cross Validation using the SVD algorithm and intial guess.  Requires that the vectors b̄ and x̄₀ are in standard form.\n\nExample Usage\n\nusing Underscores\n\nΨ = setupRegularizationProblem(A, 1)               # Setup problem\nb̄, x̄₀ = to_standard_form(Ψ, b, x₀)                 # Convert to standard form\nVλ = gcv_tr(Ψ, b̄, x̄₀, 0.1)                         # V(λ) single λ value\nVλ = @_ map(gcv_tr(Ψ, b̄, x̄₀, _), [0.1, 1.0, 10.0]) # V(λ) for array of λ\n\n\n\n\n\n","category":"function"},{"location":"library/#L-curve-Functions","page":"Library","title":"L-curve Functions","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"Lcurve_functions","category":"page"},{"location":"library/#RegularizationTools.Lcurve_functions","page":"Library","title":"RegularizationTools.Lcurve_functions","text":"Lcurve_functions(Ψ::RegularizationProblem, b̄::AbstractVector)\n\nCompute the L-curve functions to evaluate the norms L1, L2, and the curvature κ.  Requires that the vectors b̄ is in standard form.\n\nExample Usage\n\nΨ = setupRegularizationProblem(A, 1)\nb̄ = to_standard_form(Ψ, b)\nL1norm, L2norm, κ = Lcurve_functions(Ψ, b̄)\n\nL1norm.([0.1, 1.0, 10.0])    # L1 norm for λ's\nL2norm.([0.1, 1.0, 10.0])    # L2 norm for λ's\nκ.([0.1, 1.0, 10.0])         # L-curve curvature for λ's\n\n\n\n\n\nLcurve_functions(Ψ::RegularizationProblem, b̄::AbstractVector, x̄₀::AbstractVector)\n\nCompute the L-curve functions to evaluate the norms L1, L2, and the curvature κ.  Requires that the vectors b̄ and x̄₀ are in standard form.\n\nExample Usage\n\nΨ = setupRegularizationProblem(A, 1)\nb̄, x̄₀ = to_standard_form(Ψ, b, x₀)                 \nL1norm, L2norm, κ = Lcurve_functions(Ψ, b̄, x̄₀)\n\nL1norm.([0.1, 1.0, 10.0])    # L1 norm for λ's\nL2norm.([0.1, 1.0, 10.0])    # L2 norm for λ's\nκ.([0.1, 1.0, 10.0])         # L-curve curvature for λ's\n\n\n\n\n\n","category":"function"},{"location":"theory/theory/#The-Inverse-Problem","page":"Theory","title":"The Inverse Problem","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"Consider the following linear system of equations","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"bf rm bf Arm x=rm y","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"where bf rm bf A is a square design matrix, rm x is a vector of input parameters and rm y is a vector of responses. To estimate unknown inputs from response, the matrix inverse can be used","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm x=rm bf A^-1y","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"However, if a random measurement error epsilon is superimposed on rm y, i.e. b_i=y_i+epsilon_i, the estimate rm hatx from the matrix inverse ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm hatx=rm bf A^-1b","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"becomes dominated by contributions from data error for large systems. ","category":"page"},{"location":"theory/theory/#Example","page":"Theory","title":"Example","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"note: Note\nThe example system is a test problem for regularization methods is taken from MatrixDepot.jl and is the same system used in Hansen (2000).","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"Consider the following example system of 100 equations. The matrix rmbfA is 100x100.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using MatrixDepot               # hide\nr = mdopen(\"shaw\", 100, false)  # hide\nA, x, y = r.A, r.x, r.b         # hide\n\nA","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The vector rmx of input variables has 100 elements.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using MatrixDepot               # hide\nusing Random                    # hide\nr = mdopen(\"shaw\", 100, false)  # hide\nA, x, y = r.A, r.x, r.b         # hide\nRandom.seed!(302)               # hide\n\nx","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"Computing rmy and rmb using the pseudo-inverse pinv shows that the error in rmb makes the inversion unusable.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using Cairo               # hide\nusing Fontconfig          # hide\nusing RegularizationTools # hide\nusing MatrixDepot         # hide\nusing Gadfly              # hide\nusing Random              # hide\nusing DataFrames          # hide\nusing LinearAlgebra       # hide\nusing Printf              # hide\nusing Underscores         # hide\nusing Colors #hide\n\ninclude(\"helpers.jl\")     # hide\nr = mdopen(\"shaw\", 100, false)  # hide\nA, x, y = r.A, r.x, r.b         # hide\nRandom.seed!(302)               # hide\n\ny = A * x\nb = y + 0.1y .* randn(100)\nx = pinv(A) * y\nx̂ = pinv(A) * b\n# hide\nn = length(x) # hide\nd = 1:1:n # hide\ndf1 = DataFrame(x = d, y = y, Color = [\"y\" for i = 1:n]) # hide\ndf2 = DataFrame(x = d, y = b, Color = [\"b\" for i = 1:n]) # hide\ndf = [df1; df2] # hide\np1 = graph(df) # hide\ndf1 = DataFrame(x = d, y = x, Color = [\"x\" for i = 1:n]) # hide\ndf2 = DataFrame(x = d, y = x̂, Color = [\"x̂\" for i = 1:n]) # hide\np2 = graph(df1) # hide\np3 = graph(df2, colors = [\"darkred\"]) # hide\nset_default_plot_size(22cm, 7cm) # hide\nhstack(p1, p2, p3) # hide","category":"page"},{"location":"theory/theory/#Tikhonov-Regularization","page":"Theory","title":"Tikhonov Regularization","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"Tikhonov regularization is a means to filter this noise by solving the minimization problem ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm rm x_lambda=argminleft leftlVert bf rm bf Arm x-rm brightrVert _2^2+lambda^2leftlVert rm bf L(rm x-rm x_0)rightrVert _2^2right ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"where rm x_lambda is the regularized estimate of rm x, leftlVert cdotrightrVert _2 is the Euclidean norm, rm bf L is the Tikhonov filter matrix, lambda is the regularization parameter, and rm x_0 is a vector of an a priori guess of the solution. The initial guess can be taken to be rm x_0=0 if no a priori information is known. The matrix rm bf A does not need to be square. ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"For lambda=0 the Tikhonov problem reverts to the ordinary least squares solution. If rm bf A is square and lambda=0, the least-squares solution is rm hatx=rm bf A^-1b. For large lambda the solution reverts to the initial guess., i.e. lim_lambdarightarrowinftyrm x_lambda=rm x_0. Therefore, the regularization parameter lambda interpolates between the initial guess and the noisy ordinary least squares solution. The filter matrix rm bf L provides additional smoothness constraints on the solution. The simplest form is to use the identity matrix, rm bf L=rm bf I.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The formal solution to the Tikhonov problem is given by","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm x_lambda=left(rm bf A^Trm bf A+lambda^2rm bf L^Trm bf Lright)^-1left(rm bf A^Trm b+lambda^2rm bf L^Trm bf Lrm x_0right)","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The equation is readily derived by writing f=leftlVert bf rm bf Arm x-rm brightrVert _2^2+lambda^2leftlVert rm bf L(rm x-rm x_0)rightrVert _2^2, take fracdfdrm rm x=0, and solve for rm x. Use http://www.matrixcalculus.org/ to validate symbolic matrix derivatives.","category":"page"},{"location":"theory/theory/#Example-2","page":"Theory","title":"Example","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"Here is a simple regularized inversion for the same system using rm bf L=rm bf I and rm x_0=0. The regularized solution is ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm x_lambda=left(rm bf A^Trm bf A+lambda^2rm bf Iright)^-1rm bf A^Trm b","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The regularized inverse can be trivially computed assuming a value for lambda = 011.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using RegularizationTools # hide\nusing MatrixDepot # hide\nusing Gadfly # hide\nusing Random # hide\nusing DataFrames # hide\nusing Colors # hide\nusing LinearAlgebra # hide\nusing Underscores # hide\nusing Printf #  hide\n\ninclude(\"helpers.jl\") # hide\nr = mdopen(\"shaw\", 100, false) # hide\nA, x, y = r.A, r.x, r.b # hide\nRandom.seed!(716) # hide\n\ny = A * x\nb = y + 0.1y .* randn(100)\nIₙ = Matrix{Float64}(I, 100, 100)\nλ = 0.11\nxλ = inv(A'A + λ^2.0 * Iₙ) * A' * b\n\nn = length(x) # hide\nd = 1:1:n # hide\ndf1 = DataFrame(x = d, y = y, Color = [\"y\" for i = 1:n]) # hide\ndf2 = DataFrame(x = d, y = b, Color = [\"b\" for i = 1:n]) # hide\ndf = [df1; df2] # hide\np1 = graph(df) # hide\n# hide\ndf1 = DataFrame(x = d, y = x, Color = [\"x\" for i = 1:n]) # hide\ndf2 = DataFrame(x = d, y = xλ, Color = [\"xλ\" for i = 1:n]) # hide\ndf = [df1; df2] # hide\np2 = graph(df) # hide\nset_default_plot_size(15cm, 6cm) # hide\nhstack(p1, p2) # hide","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The solution is not perfect, but it is free of random error and a reasonable approximation of the true rmx.","category":"page"},{"location":"theory/theory/#Optimal-Regularization-Parameter","page":"Theory","title":"Optimal Regularization Parameter","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The choice of the optimal regularization parameter is not obvious. If we pick lambda too small, the solution is dominated by noise. If we pick lambda too large the solution will not approximate the correct solution.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using RegularizationTools # hide\nusing MatrixDepot # hide\nusing Gadfly # hide\nusing Random # hide\nusing DataFrames # hide\nusing Colors # hide\nusing LinearAlgebra # hide\nusing Underscores #hide\nusing Printf #  hide\n\ninclude(\"helpers.jl\") # hide\nr = mdopen(\"shaw\", 100, false) # hide\nA, x, y = r.A, r.x, r.b # hide\nRandom.seed!(716) # hide\n\ny = A * x\nb = y + 0.1y .* randn(100)\nIₙ = Matrix{Float64}(I, 100, 100)\nf(λ) = inv(A'A + λ^2.0 * Iₙ) * A' * b\nxλ1 = f(0.001)\nxλ2 = f(0.1) \nxλ3 = f(10.0)\n\nn = length(x) # hide\nd = 1:1:n # hide\ndf1 = DataFrame(x = d, y = y, Color = [\"y\" for i = 1:n]) # hide\ndf2 = DataFrame(x = d, y = b, Color = [\"b\" for i = 1:n]) # hide\ndf = [df1; df2] # hide\np1 = graph(df) # hide\n# hide\ndf1 = DataFrame(x = d, y = x, Color = [\"x\" for i = 1:n]) # hide\ndf2 = DataFrame(x = d, y = xλ1, Color = [\"xλ1\" for i = 1:n]) # hide\ndf3 = DataFrame(x = d, y = xλ2, Color = [\"xλ2\" for i = 1:n]) # hide\ndf4 = DataFrame(x = d, y = xλ3, Color = [\"xλ3\" for i = 1:n]) # hide\ndf = [df1; df2; df3; df4] # hide\np2 = graph(df; colors = [\"black\", \"steelblue3\", \"darkred\", \"darkgoldenrod\"]) # hide\nset_default_plot_size(15cm, 6cm) # hide\nhstack(p1, p2) # hide","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"lambda = 01 provides an acceptable solution. lambda = 0001 is noisy (under-regularized) and lambda = 100 is incorrect (over-regularized). There are several objective methods to find the optimal regularization parameter. The general procedure to identify the optimal lambda is to compute rm x_lambda for a range of regularization parameters over the interval [lambda_1, lambda_2] and then apply some evaluation criterion that objectively evaluates the quality of the solution. This package implements two of these, the L-curve method and generalized cross validation.","category":"page"},{"location":"theory/theory/#L-Curve-Method","page":"Theory","title":"L-Curve Method","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The L-curve method evaluates the by balancing the size of the residual norm L_1=leftlVert bf rm bf Arm x_lambda-rm brightrVert _2 and the size of the solution norm L_2=leftlVert rm bf L(rm x_lambda-rm x_0)rightrVert _2 for rm x_lambdainlambda_1lambda_2. The L-curve consists of a plot of log L_1 vs. log L_1. The following example illustrates the L-curve without specifying an a priori input.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using MatrixDepot # hide\nusing Gadfly  # hide\nusing Random # hide\nusing DataFrames # hide\nusing Colors # hide\nusing LinearAlgebra # hide\nusing Underscores # hide\nusing NumericIO # hide\nusing Printf # hide\n\ninclude(\"helpers.jl\") # hide\nr = mdopen(\"shaw\", 100, false) # hide\nA, x, y = r.A, r.x, r.b # hide\nRandom.seed!(716) # hide\n\ny = A * x\nb = y + 0.1y .* randn(100)\nIₙ = Matrix{Float64}(I, 100, 100)\nf(λ) = inv(A'A + λ^2.0 * Iₙ) * A' * b\nL1(λ) = norm(A * f(λ) - b)\nL2(λ) = norm(Iₙ * f(λ))\nλs = exp10.(range(log10(1e-5), stop = log10(10), length = 100))\nresidual, solution = L1.(λs), L2.(λs)\n\nset_default_plot_size(5inch, 3.5inch) # hide\ngraph1(residual, solution) # hide","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The optimal lambda_opt is the corner of the L-curve. In this example this is lambda_opt approx 01, which yielded the acceptable solution earlier. Finding the corner of the L-curve can be automated by performing an gradient descent search to find the mimum value of the curvature of the L-curve (Hansen, 2000). The implementation is discussed in the L-Curve Algorithm section.","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The solve function in RegularizationTools can be used to find λopt through the L-curve algorithm, searching over the predefined interval [lambda_1, lambda_2].","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using MatrixDepot # hide\nusing Lazy   # hide\nusing Random # hide\nusing RegularizationTools\n\nr = mdopen(\"shaw\", 100, false) #hide\nA, x, y = r.A, r.x, r.b   # hide\nRandom.seed!(716)  # hide\ny = A * x\nb = y + 0.1y .* randn(100)\nsolution = @> setupRegularizationProblem(A, 0) solve(b, alg = :L_curve, λ₁ = 0.01, λ₂ = 1.0)\nλopt = solution.λ","category":"page"},{"location":"theory/theory/#Generalized-Cross-Validation","page":"Theory","title":"Generalized Cross Validation","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"If the general form of the problem ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm rm x_lambda=argminleft leftlVert bf rm bf Arm x-rm brightrVert _2^2+lambda^2leftlVert rm bf L(rm x-rm x_0)rightrVert _2^2right","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"has the smoothing matrix rm bf L=rm bf I, the problem is considered to be in standard form. The general-form problem can be transformed into standard form (see Transformation to Standard Form for algorithm). If the problem is in standrad form, and if rm x_0=0, the GCV estimate of lambda is (Golub et al., 1979):","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"V(lambda)=fracnleftlVert (bf rm bf bf I-bf A_lambda)rm brightrVert _2^2tr(rm bf I-rm bf A_lambda)^2","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"where bf A_lambda=rm rm bf Aleft(bf AA^T-lambda^2rm bf Iright)^-1bf A^T is the influence matrix, tr is the matrix trace, n is the size of rmb. Note that rm x_lambda=left(rm bf A^Trm bf A+lambda^2rm bf Iright)^-1rm bf A^Trm b. Therefore rm rm rm bf Ax_lambda=rm bf A_lambdab and leftlVert (bf rm bf bf I-bf A_lambda)rm brightrVert _2=leftlVert bf rm bf Arm x_lambda-rm brightrVert _2. The optimal lambda_opt coincides with the global minimum of V(lambda). ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The following example evaluates V(lambda) over a range of lambda. ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using MatrixDepot #hide\nusing Gadfly #hide\nusing Random #hide\nusing Colors #hide\nusing DataFrames #hide\nusing Printf#hide\nusing Lazy#hide\nusing Underscores #hide\nusing LinearAlgebra #hide\ninclude(\"helpers.jl\") #hide\nr = mdopen(\"shaw\", 100, false) #hide\nA, x, y = r.A, r.x, r.b #hide\nRandom.seed!(716) #hide\ny = A * x\nb = y + 0.1y .* randn(100)\nIₙ = Matrix{Float64}(I, 100, 100)\nAλ(λ) = A*inv(A'A + λ^2.0*Iₙ)*A'\ngcv(λ) = 100*norm((Iₙ - Aλ(λ))*b)^2.0/tr(Iₙ - Aλ(λ))^2.0\nλs = exp10.(range(log10(1e-3), stop = log10(1), length = 100))\nV = map(gcv, λs)\n#hide\nset_default_plot_size(5inch, 3.5inch) # hide\ngraph2(λs, V) #hide","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The GCV curve has a steep part for large lambda and a shallow part for small lambda. The minimum occurs near lambda = 01.  The solve function in RegularizationTools can be used to find λopt through the GCV approach, searching over the predefined interval [lambda_1, lambda_2].","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"using MatrixDepot # hide\nusing Lazy   # hide\nusing Random # hide\nusing RegularizationTools\n\nr = mdopen(\"shaw\", 100, false) #hide\nA, x, y = r.A, r.x, r.b   # hide\nRandom.seed!(716)  # hide\ny = A * x\nb = y + 0.1y .* randn(100)\nsolution = @> setupRegularizationProblem(A, 0) solve(b, alg = :gcv_svd, λ₁ = 0.01, λ₂ = 1.0)\nλopt = solution.λ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"Note that the objective λopt from the L-curve and GCV criterion are nearly identical. ","category":"page"},{"location":"theory/theory/#Transformation-to-Standard-Form","page":"Theory","title":"Transformation to Standard Form","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The general from couched in terms of rmbfA rm b rm x rm x_0 and rmbfL","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm rm x_lambda=argminleft leftlVert bf rm bf Arm x-rm brightrVert _2^2+lambda^2leftlVert rm bf L(rm x-rm x_0)rightrVert _2^2right","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"is transformed to standard form couched in terms of rm bf bar A rm bar b rm bar x rm barx_0 and rmbfI","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm rm x_lambda=argminleft leftlVert bf rm bf bar Arm x-rm bar brightrVert _2^2+lambda^2leftlVert rm bf I(rm bar x-rm bar x_0)rightrVert _2^2right","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"as introduced by Eldén (1977) using notation from Hansen (1998, Chapter 2.3.1). The algorithm computes the explicit transformation using two QR factorizations. The matrices needed for the explicit conversion depend on rm bf A and rm bf L and are computed and cached in setupRegularizationProblem. ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The λopt search is performed in in standard form, and the solution is computed in standard form. Then the resulting solution is transformed back to the general form using the same matrices.","category":"page"},{"location":"theory/theory/#Solving-the-Standard-Equations","page":"Theory","title":"Solving the Standard Equations","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The solution rm bar x_lambda for the transformed standard equation is ","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm bar x_lambda=left(rm bf bar A^Trm bf bar A+lambda^2rm bf Iright)^-1 left( rm bf bar A^Trm bar b + λ^2 rm bar x_0 right)","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"rm bar x_lambda is found using the Cholesky decomposition. It is the alorithm used in the MultivariateStats package and generally faster than the QR approach (Lira et al., 2016).","category":"page"},{"location":"theory/theory/#L-Curve-Algorithm","page":"Theory","title":"L-Curve Algorithm","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The curvature kappa(lambda) of the L-curve is computed using Eq. (18) in Hansen (2000). The expression requires calculation of the solution and residual norms, as well as the first derivative of the solution norm. The derivative is calculated using finite differences from the Calculus package The corner of the L-curve occurs when the curvature maximizes. Finally,  λ_opt is found by minimizing the -kappa(lambda) function (see L-Curve Method) on a bounded interval using Brent's method, as implemented in the Optim package.","category":"page"},{"location":"theory/theory/#Generalized-Cross-Validation-Algorithm","page":"Theory","title":"Generalized Cross Validation Algorithm","text":"","category":"section"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"V(lambda)=fracnleftlVert (bf rm bf bf I-bf bar A_lambda)rm bar brightrVert _2^2tr(rm bf I-rm bf bar A_lambda)^2","category":"page"},{"location":"theory/theory/","page":"Theory","title":"Theory","text":"The slowest part in the GCV calculation is evaluation of the trace.  The GCV estimate is computed either using the single value decomposition (SVD) algorithm (Golub et al., 1979) (gcv_svd) or the explicit calculation using the trace term (gcv_tr). The SVD of the design matrix in standard form rm bfbar A is calculated and cached in setupRegularizationProblem. When an initial guess is included, the denominator is computed using the SVD estimate and the numerator is computed via leftlVert bf rm bf bar Arm bar x_lambda-rm bar brightrVert _2^2 and rm bar x_lambda is obtained using the Cholesky decomposition algorithm solving for the Tikhonov solution in standard form with an initial guess. Note that this is an approximation because the trace term in the denominator does not account for the intial guess. Comparison with the L-curve method suggest that this approximation does not affect the quality of the regularized solution. Finally, λ_opt is found by minimizing V(lambda) on a bounded interval using Brent's method, as implemented in the Optim package.","category":"page"},{"location":"manual/#Manual","page":"Manual","title":"Manual","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"A theoretical background of Tikhonov regularization is provided in the The Inverse Problem section. ","category":"page"},{"location":"manual/#Solving-a-Problem","page":"Manual","title":"Solving a Problem","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"A problem consists of a design matrix rm bf A and a vector rm b such that","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"rm b = bf rm bf Arm x + epsilon","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where epsilon is noise and the objective is to reconstruct the original parameters rm x. The solution to the problem is to minimize","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"rm rm x_lambda=argminleft leftlVert bf rm bf Arm x-rm brightrVert _2^2+lambda^2leftlVert rm bf L(rm x-rm x_0)rightrVert _2^2right ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where rm x_lambda is the regularized estimate of rm x, leftlVert cdotrightrVert _2 is the Euclidean norm, rm bf L is the Tikhonov filter matrix, lambda is the regularization parameter, and rm x_0 is a vector of an a priori guess of the solution. The initial guess can be taken to be rm x_0=0 if no a priori information is known. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The basic steps to solve the problem are ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Ψ = setupRegularizationProblem(A, 2)   # Setup the problem \nsolution = solve(Ψ, b)                 # Compute the solution \nxλ = solution.x                        # Extract the x","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The solve function finds Optimal Regularization Parameter, by default using Generalized Cross Validation. It applies the optimal lambda value to compute rm x_lambda. The solution is of type RegularizatedSolution, which contains the optimal solution (solution.x), the optimal lambda (solution.λ) and output from the otimization routine (solution.solution).","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"An convenient way to find x is using Lazy pipes:","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"xλ = @> setupRegularizationProblem(A, 2) solve(b) getfield(:x)","category":"page"},{"location":"manual/#Specifying-the-Order","page":"Manual","title":"Specifying the Order","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Common choices for the bfrmL matrix are finite difference approximations of a derivative. There are termed zeroth, first, and second order inversion matrices. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"rm bf L_0=left(beginarrayccccc\n1        0\n  1\n    ddots\n      1\n0        1\nendarrayright)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"rm bf L_1=left(beginarraycccccc\n1  -1        0\n  1  -1\n    ddots  ddots\n      1  -1\n0        1  -1\nendarrayright)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"rm bf L_2=left(beginarrayccccccc\n-1  2  -1        0\n  -1  2  -1\n    ddots  ddots  ddots\n      -1  2  -1\n0        -1  2  -1\nendarrayright)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"You can specify which of these matrices to use in ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"setupRegularizationProblem(A::AbstractMatrix, order::Int)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where order = 0, 1, 2 corresponds to bfrmL_0, bfrmL_1, and  bfrmL_2","category":"page"},{"location":"manual/#Example-:-[Phillips-Problem](https://matrixdepotjl.readthedocs.io/en/latest/regu.html#term-phillips)","page":"Manual","title":"Example : Phillips Problem","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Example with 100 point discretization and zero initial guess.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, MatrixDepot, Lazy\nusing Random # hide\n\nr = mdopen(\"phillips\", 100, false)\nA, x = r.A, r.x\nRandom.seed!(850) # hide\n\ny = A * x\nb = y + 0.1y .* randn(100)\nxλ = @> setupRegularizationProblem(A, 2) solve(b) getfield(:x)\ninclude(\"theory/helpers.jl\") # hide\nstandard_plot(y, b, x, xλ, 0.0x) # hide","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"note: Note\nThe random perturbation b = y + 0.1y .* randn(100)  in each of the examples uses a fixed random seed to ensure reproducibility. The random seed and plot commands are hidden for clarity. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"note: Note\nThe example system is a test problem for regularization methods is taken from MatrixDepot.jl and is the same system used in Hansen (2000).","category":"page"},{"location":"manual/#Example-2:-[Shaw-Problem](https://matrixdepotjl.readthedocs.io/en/latest/regu.html#term-shaw)","page":"Manual","title":"Example 2: Shaw Problem","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Zeroth order example with 500 point discretization and moderate initial guess.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, MatrixDepot, Lazy, Random\n\nr = mdopen(\"shaw\", 500, false)\nA, x = r.A, r.x\nRandom.seed!(850) #hide \n\ny = A * x\nb = y + 0.1y .* randn(500)\nx₀ = 0.6x\n\nxλ = @> setupRegularizationProblem(A, 0) solve(b, x₀) getfield(:x)\ninclude(\"theory/helpers.jl\") # hide\nstandard_plot(y, b, x, xλ, x₀)# hide","category":"page"},{"location":"manual/#Using-a-Custom-L-Matrix","page":"Manual","title":"Using a Custom L Matrix","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"You can specify custom rm bf L matrices when setting up problems.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"setupRegularizationProblem(A::AbstractMatrix, L::AbstractMatrix)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"For example, Huckle and Sedlacek (2010) propose a two-step data based regularization","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"rm bf L = rm bf L_k rm bf D_hatx^-1 ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"where rm bf L_k is one of the finite difference approximations of a derivative,  rm bf D_hatx=diag(hatx_1ldotshatx_n), hatx is the reconstruction of x using rm bf L_k, and (rm bf D_hatx)_ii=epsilonforallhatx_iepsilon, with epsilon  1. ","category":"page"},{"location":"manual/#Example-3:-[Heat-Problem-](https://matrixdepotjl.readthedocs.io/en/latest/regu.html#term-heat)","page":"Manual","title":"Example 3: Heat Problem ","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"This examples illustrates how to implement the Huckle and Sedlacek (2010) matrix. Note that Γ(A, 2) returns the Tikhonov Matrix of order 2. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, MatrixDepot, Lazy, Random, LinearAlgebra\n\nr = mdopen(\"heat\", 100, false)\nA, x = r.A, r.x\nRandom.seed!(150) #hide\n\ny = A * x\nb = y + 0.05y .* randn(100)\nx₀ = zeros(length(b))\n\nL₂ = Γ(A,2)                  \nxλ1 = @> setupRegularizationProblem(A, L₂) solve(b) getfield(:x)\nx̂ = deepcopy(abs.(xλ1))\nx̂[abs.(x̂) .< 0.1] .= 0.1\nL = L₂*Diagonal(x̂)^(-1)\nxλ2 = @> setupRegularizationProblem(A, L) solve(b) getfield(:x)\ninclude(\"theory/helpers.jl\") # hide\nstandard_plot1(y, b, x, xλ1, xλ2) # hide","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The solution xλ2 is improved over the regular L₂ solution. ","category":"page"},{"location":"manual/#Customizing-the-Search-Algorithm","page":"Manual","title":"Customizing the Search Algorithm","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"The solve function searches for the optimum regularization parameter lambda between lambda_1 lambda_2. The default search range is [0.001, 1000.0] and the interval range can be modified through keyword parameters. The optimality criterion is either the minimum of the Generalized Cross Validation function, or the the maximum curvature of the L-curve (see L-Curve Algorithm). The algorithm can be specified through the alg keyword. Valid algorithms are :L_curve, :gcv_svd, and :gcv_tr (see Solve).","category":"page"},{"location":"manual/#Example:-[Baart-Problem](https://matrixdepotjl.readthedocs.io/en/latest/regu.html#term-baart)","page":"Manual","title":"Example: Baart Problem","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, MatrixDepot, Lazy, Random\n\nr = mdopen(\"baart\", 100, false)\nA, x = r.A, r.x\nRandom.seed!(150) #hide\n\ny = A  * x\nb = y + 0.1y .* randn(100)\n\nxλ1 = @> setupRegularizationProblem(A,1) solve(b, alg=:L_curve, λ₁=100.0, λ₂=1e6) getfield(:x)\nxλ2 = @> setupRegularizationProblem(A,1) solve(b, alg=:gcv_svd) getfield(:x)\ninclude(\"theory/helpers.jl\") # hide\nx₀ = 0.0*x # hide\nstandard_plot1(y, b, x, xλ1, xλ2) # hide","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Note that the output from the L-curve and GCV algorithm are nearly identical. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"note: Note\nThe L-curve algorithm is more sensitive to the bounds and slower than the gcv_svd algorithm. There may, however, be cases where the L-curve approach is preferable. ","category":"page"},{"location":"manual/#Extracting-the-Validation-Function","page":"Manual","title":"Extracting the Validation Function","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"The solution is obtained by first transforming the problem to standard form (see Transformation to Standard Form). The following example can be used to extract the GCV function.","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, MatrixDepot, Lazy, Random, Underscores, Printf\n\nr = mdopen(\"shaw\", 100, false)\nA, x = r.A, r.x\nRandom.seed!(716) #hide\n\ny = A  * x\nb = y + 0.1y .* randn(100)\n\nΨ = setupRegularizationProblem(A,1)\nλopt = @> solve(Ψ, b, alg=:gcv_svd) getfield(:λ)\nb̄ = to_standard_form(Ψ, b) \nλs = exp10.(range(log10(1e-1), stop = log10(10), length = 100))\nVλ = @_ map(gcv_svd(Ψ, b̄, _), λs) \ninclude(\"theory/helpers.jl\") # hide\ngraph3(λs, Vλ) # hide","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The calculated λopt from ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"λopt = @> solve(Ψ, b, alg=:gcv_svd) getfield(:λ)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"is 1.1 and corresponds to the minimum of the GCV curve. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"Alternatively, the L-curve is retrieved through the L-curve Functions","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, MatrixDepot, Lazy, Random, Underscores, Printf\n\nr = mdopen(\"shaw\", 100, false)\nA, x = r.A, r.x\nRandom.seed!(716) #hide\n\ny = A  * x\nb = y + 0.1y .* randn(100)\n\nΨ = setupRegularizationProblem(A,1)\nλopt = @> solve(Ψ, b, alg=:L_curve, λ₂ = 10.0) getfield(:λ)\nb̄ = to_standard_form(Ψ, b) \nλs = exp10.(range(log10(1e-3), stop = log10(100), length = 200))\nL1norm, L2norm, κ = Lcurve_functions(Ψ, b̄)\nL1, L2 = L1norm.(λs), L2norm.(λs)    \ninclude(\"theory/helpers.jl\") # hide\ngraph4(L1, L2) # hide","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"The calculated λopt from ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"λopt = @> solve(Ψ, b, alg=:L_curve, λ₂ = 10.0) getfield(:λ)","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"is 0.9 and corresponds to the corner of the L-curve.","category":"page"},{"location":"manual/#Benchmarks","page":"Manual","title":"Benchmarks","text":"","category":"section"},{"location":"manual/","page":"Manual","title":"Manual","text":"Systems up to a 1000 equations are unproblematic. The setup for much larger system slows down due to the approx O(n^2) (or worse) time complexity of the SVD and generalized SVD factorization of the design matrix. Larger systems require switching to SVD free algorithms, which are currently not supported by this package. ","category":"page"},{"location":"manual/","page":"Manual","title":"Manual","text":"using RegularizationTools, TimerOutputs, MatrixDepot\n\nto = TimerOutput()\n\nfunction benchmark(n)\n    r = mdopen(\"shaw\", n, false)\n    A, x = r.A, r.x\n    y = A * x\n    b = y + 0.05y .* randn(n)\n    Ψ = setupRegularizationProblem(A, 2)\n    for i = 1:1\n        @timeit to \"Setup  (n = $n)\" setupRegularizationProblem(A, 2)\n        @timeit to \"Invert (n = $n)\" solve(Ψ, b)\n    end\nend\n\nmap(benchmark, [10, 100, 1000])\nshow(to)","category":"page"},{"location":"#RegularizationTools.jl","page":"Home","title":"RegularizationTools.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package to perform Tikhonov regularization for small to moderate size problems.","category":"page"},{"location":"","page":"Home","title":"Home","text":"RegularizationTools.jl bundles a set routines to compute the regularized  Tikhonov inverse using standard linear algebra techniques.  ","category":"page"},{"location":"#Package-Features","page":"Home","title":"Package Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Computes the Tikhonov inverse solution.\nComputes optimal regularization parameter using generalized cross validation or the L-curve.\nSolves problems with up to a 1000 equations.\nSupports zero, first, and second order regularization out of the box.\nSupports specifying an a-priori estimate of the solution.\nSupports user specified smoothing matrices.\nUser friendly interface.\nExtensive documentation.","category":"page"},{"location":"#About","page":"Home","title":"About","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Tikhonv regularization is also known as Phillips-Twomey-Tikhonov regularization or ridge regression (see Hansen, 2000 for a review). The Web-of-Sciences database lists more than 4500 peer-reviewed publications mentioning \"Tikhonov regularization\" in the title or abstract, with a current publication rate of ≈350 new papers/year. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The first draft of this code was part of my DifferentialMobilityAnalyzers package. Unfortunately, the initial set of algorithms were too limiting and too slow. I needed a better set of regularization tools to work with, which is how this package came into existence. Consequently, the scope of the package is defined by my need to support data inversions for the DifferentialMobilityAnalyzers project. My research area is not on inverse methods and I currently do not intend to grow this package into something that goes much beyond the implemented algorithms. However, the code is a generic implementation of the Tikhonov method and might be useful to applied scientists who need to solve standard ill-posed inverse problems that arise in many disciplines. ","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package computes the regularized Tikhonov inverse rm x_lambda by solving the minimization problem ","category":"page"},{"location":"","page":"Home","title":"Home","text":"rm rm x_lambda=argminleft leftlVert bf rm bf Arm x-rm brightrVert _2^2+lambda^2leftlVert rm bf L(rm x-rm x_0)rightrVert _2^2right ","category":"page"},{"location":"","page":"Home","title":"Home","text":"where rm x_lambda is the regularized estimate of rm x, leftlVert cdotrightrVert _2 is the Euclidean norm, rm bf L is the Tikhonov filter matrix, lambda is the regularization parameter, and rm x_0 is a vector of an a-priori guess of the solution. The initial guess can be taken to be rm x_0=0 if no a-priori information is known. The solve function searches for the optimal lambda and returns the inverse. The following script is a minimalist example how to use this package.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using RegularizationTools, MatrixDepot, Lazy\nusing Random #hide\n\n# This is a test problem for regularization methods\nr = mdopen(\"shaw\", 100, false)       # Load the \"shaw\" problem from MatrixDepot\nA, x  = r.A, r.x                     # A is size(100,100), x is length(100)\nRandom.seed!(716)  # hide\n\ny = A * x                            # y is the true response \nb = y + 0.2y .* randn(100)           # response with superimposed noise\nx₀ = 0.4x                            # some a-priori estimate x₀\n\n# Solve 2nd order Tikhonov inversion (L = uppertridiag(−1, 2, −1)) with intial guess x₀\nxλ = @> setupRegularizationProblem(A, 2) solve(b, x₀) getfield(:x)\ninclude(\"theory/helpers.jl\") # hide\nstandard_plot(y, b, x, xλ, x₀) # hide","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package can be installed from the Julia package prompt with","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add RegularizationTools","category":"page"},{"location":"","page":"Home","title":"Home","text":"The closing square bracket switches to the package manager interface and the add command installs the package and any missing dependencies. To return to the Julia REPL hit the delete key.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To load the package run","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using RegularizationTools","category":"page"},{"location":"","page":"Home","title":"Home","text":"For optimal performance, also install the Intel MKL linear algebra library.","category":"page"},{"location":"#Related-work","page":"Home","title":"Related work","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MultivariateStats: Implements ridge regression without a priori estimate and does not include tools to find the optimal regularization parameter.\nRegularizedLeastSquares: Implements optimization techniques for large-scale scale linear systems.","category":"page"},{"location":"#Author-and-Copyright","page":"Home","title":"Author and Copyright","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Markus Petters, Department of Marine, Earth, and Atmospheric Sciences, NC State University.","category":"page"}]
}
